{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required packages\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List\n",
    "import csv\n",
    "\n",
    "class SignLanguageMNIST(Dataset):\n",
    "    \"\"\"Sign Language classification dataset for A-Y without J (J and Z require movement)\n",
    "    Each sample is 1 x 1 x 28 x 28, and each label is a scalar.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_label_mapping():\n",
    "        \"\"\"\n",
    "        Return the label mapping which contains a list 0-24 without 9\n",
    "        \"\"\"\n",
    "        mapping = list(range(25))\n",
    "        mapping.pop(9)\n",
    "        return mapping\n",
    "\n",
    "    @staticmethod\n",
    "    def read_label_samples_from_csv(path: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: Path to dataset (.csv file) \n",
    "        First column in CSV is the label and remaining 28x28 values\n",
    "        are image pixel values.\n",
    "        Returns:\n",
    "            labels: labels got from the mapping function\n",
    "            samples: all the samples in the csv file \n",
    "        \"\"\"\n",
    "        mapping = SignLanguageMNIST.get_label_mapping()\n",
    "        labels, samples = [], []\n",
    "        with open(path) as f:\n",
    "            _ = next(f)  # skip header\n",
    "            for line in csv.reader(f):\n",
    "                label = int(line[0])\n",
    "                labels.append(mapping.index(label))\n",
    "                samples.append(list(map(int, line[1:])))\n",
    "        return labels, samples\n",
    "\n",
    "    def __init__(self,\n",
    "            path: str=\"data/sign_mnist_train.csv\",\n",
    "            mean: List[float]=[0.485],\n",
    "            std: List[float]=[0.229]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: Path to dataset (.csv file) \n",
    "            mean: Mean of the dataset, has value set default\n",
    "            std: Standard Deviation of the dataset, has value set default`\n",
    "        \"\"\"\n",
    "        labels, samples = SignLanguageMNIST.read_label_samples_from_csv(path)\n",
    "        self._samples = np.array(samples, dtype=np.uint8).reshape((-1, 28, 28, 1))\n",
    "        self._labels = np.array(labels, dtype=np.uint8).reshape((-1, 1))\n",
    "\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: index of the image \n",
    "        Apply transforms to convert to PILImage, apply crop,\n",
    "        convert to tensor and normalize the tensor.\n",
    "        Returns:\n",
    "            image,label: transformes image and its respective label \n",
    "        \"\"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomResizedCrop(28, scale=(0.8, 1.2)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=self._mean, std=self._std)])\n",
    "\n",
    "        return {\n",
    "            'image': transform(self._samples[idx]).float(),\n",
    "            'label': torch.from_numpy(self._labels[idx]).float()\n",
    "        }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_loaders(batch_size=32):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        batch_size: optional argument, default is 32\n",
    "    Creates training and testing dataloaders.\n",
    "        \"\"\"\n",
    "    trainset = SignLanguageMNIST('C:/Users/12134/Downloads/3258_5337_bundle_archive/sign_mnist_train.csv')\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    testset = SignLanguageMNIST('C:/Users/12134/Downloads/3258_5337_bundle_archive/sign_mnist_test.csv')\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[[ 0.1768,  0.2453,  0.3823,  ...,  0.8789,  0.8276,  0.8276],\n",
      "          [ 0.2282,  0.3309,  0.4851,  ...,  0.9132,  0.8618,  0.8447],\n",
      "          [ 0.3138,  0.4337,  0.5878,  ...,  0.9646,  0.9474,  0.8961],\n",
      "          ...,\n",
      "          [ 1.2899,  1.4612,  1.5639,  ..., -1.6384, -1.3302, -1.2617],\n",
      "          [ 1.3242,  1.4954,  1.6324,  ..., -1.5014, -1.5528, -1.3473],\n",
      "          [ 0.6563,  0.7933,  0.8789,  ..., -1.3815, -1.4672, -1.5185]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9474,  0.9474,  0.9646,  ...,  0.0398,  0.6221,  0.5193],\n",
      "          [ 0.9988,  0.9988,  0.9988,  ..., -0.2684,  0.6392,  0.5707],\n",
      "          [ 1.0502,  1.0331,  1.0331,  ..., -0.1314,  0.7419,  0.5878],\n",
      "          ...,\n",
      "          [-0.2171, -0.1999, -0.1486,  ..., -1.7925, -1.7240, -1.8439],\n",
      "          [-0.5596, -0.5938, -0.5938,  ..., -1.7754, -1.7069, -1.9809],\n",
      "          [-0.2684, -0.2513, -0.3027,  ..., -1.8439, -1.7069, -2.1008]]]]), 'label': tensor([[ 8.],\n",
      "        [21.]])}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    loader, _ = get_train_test_loaders(2)\n",
    "    print(next(iter(loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CNN based Model architecture\"\"\"\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 6, 3)\n",
    "        self.conv3 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 48)\n",
    "        self.fc3 = nn.Linear(48, 24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train model for 20 epochs, with a learning rate of 0.01, 0.9 momentum, \n",
    "SGD optimizer over Cross Entropy Loss and save the trained model\"\"\"\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def main():\n",
    "    net = Net().float()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    trainloader, _ = get_train_test_loaders()\n",
    "    for epoch in range(20):  \n",
    "        train(net, criterion, optimizer, trainloader, epoch)\n",
    "    torch.save(net.state_dict(), \"checkpoint.pth\")\n",
    "\n",
    "\n",
    "def train(net, criterion, optimizer, trainloader, epoch):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs = Variable(data['image'].float())\n",
    "        labels = Variable(data['label'].long())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels[:, 0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.6f' % (epoch, i, running_loss / (i + 1)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PyTorch ==========\n",
      "Training accuracy: 99.5\n",
      "Validation accuracy: 96.8\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate(outputs: Variable, labels: Variable) -> float:\n",
    "    \"\"\"Evaluate model outputs \"\"\"\n",
    "    Y = labels.numpy()\n",
    "    Yhat = np.argmax(outputs, axis=1)\n",
    "    return float(np.sum(Yhat == Y))\n",
    "\n",
    "\n",
    "def batch_evaluate(\n",
    "        net: Net,\n",
    "        dataloader: torch.utils.data.DataLoader) -> float:\n",
    "    \"\"\"Evaluate model in batches, if dataset is too large.\"\"\"\n",
    "    score = n = 0.0\n",
    "    for batch in dataloader:\n",
    "        n += len(batch['image'])\n",
    "        outputs = net(batch['image'])\n",
    "        if isinstance(outputs, torch.Tensor):\n",
    "            outputs = outputs.detach().numpy()\n",
    "        score += evaluate(outputs, batch['label'][:, 0])\n",
    "    return score / n\n",
    "\n",
    "\n",
    "def validate():\n",
    "    trainloader, testloader = get_train_test_loaders()\n",
    "    net = Net().float().eval()\n",
    "\n",
    "    pretrained_model = torch.load(\"checkpoint.pth\")\n",
    "    net.load_state_dict(pretrained_model)\n",
    "\n",
    "    print('=' * 10, 'PyTorch', '=' * 10)\n",
    "    train_acc = batch_evaluate(net, trainloader) * 100.\n",
    "    print('Training accuracy: %.1f' % train_acc)\n",
    "    test_acc = batch_evaluate(net, testloader) * 100.\n",
    "    print('Validation accuracy: %.1f' % test_acc)\n",
    "\n",
    "    trainloader, testloader = get_train_test_loaders(1)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing on custom individual data\"\"\"\n",
    "\n",
    "import cv2\n",
    "\n",
    "index_to_letter = list('ABCDEFGHIKLMNOPQRSTUVWXY')\n",
    "\n",
    "img = cv2.imread('C:/Users/12134/OneDrive/Pictures/Test/V.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "pixels = cv2.resize(gray,(28,28))\n",
    "pixels = pixels-np.min(pixels)/(np.max(pixels)-np.min(pixels))\n",
    "data = pixels.reshape(1, 1, 28, 28).astype(np.float32)\n",
    "\n",
    "model = Net().float().eval()\n",
    "pretrained_model = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(pretrained_model)\n",
    "model.eval()\n",
    "x = torch.FloatTensor(data)\n",
    "y = model(Variable(x))\n",
    "pred = torch.argmax(y).cpu().numpy()\n",
    "print(index_to_letter[int(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
